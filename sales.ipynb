{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Y2dKM7QzrUkj",
        "outputId": "36858313-8bac-4cf6-f5cf-9e90eb5d4eb8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/Walmart_Sales.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3006011057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# 1. load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Walmart_Sales.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekly_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Walmart_Sales.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "import yfinance as yf\n",
        "# 1. load dataset\n",
        "df=pd.read_csv('/content/Walmart_Sales.csv', parse_dates=['Date'])\n",
        "df = df[['Weekly_Sales']]\n",
        "df.head()\n",
        "\n",
        "# 2. Scale data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "\n",
        "# 3. Create sequences\n",
        "def create_sequences(dataset, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(dataset)):\n",
        "        X.append(data[i-window_size:i, 0])\n",
        "        y.append(data[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "window_size = 30\n",
        "X, y = create_sequences(scaled_sales, window_size)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# 4. Split into training and testing\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "# 5. Build Simple RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(50, activation='tanh', input_shape=(window_size, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# 6. Train\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# 7. Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(y_pred)  # Fixed function name\n",
        "y_actual = scaler.inverse_transform(y_test.reshape(-1, 1))  # Also fixed\n",
        "\n",
        "# 8. Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_actual, label='Actual')\n",
        "plt.plot(y_pred, label='Predicted')\n",
        "plt.title('Stock Price Prediction using Simple RNN')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4ImqII9s6Cg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U390ZpVys6FV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv('/content/Walmart_Sales.csv', parse_dates=['Date'])\n",
        "df = df[['Weekly_Sales']]\n",
        "\n",
        "# 2. Scale data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_sales = scaler.fit_transform(df)\n",
        "\n",
        "# 3. Create sequences\n",
        "def create_sequences(dataset, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(dataset)):\n",
        "        X.append(dataset[i-window_size:i, 0])\n",
        "        y.append(dataset[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "window_size = 30\n",
        "X, y = create_sequences(scaled_sales, window_size)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# 4. Split into training and testing\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "# 5. Build Simple RNN model\n",
        "model = Sequential([\n",
        "    SimpleRNN(50, activation='tanh', input_shape=(window_size, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# 6. Train\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# 7. Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# 8. Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_actual, label='Actual Walmart Sales')\n",
        "plt.plot(y_pred, label='Predicted Walmart Sales')\n",
        "plt.title('Walmart Sales Prediction using Simple RNN')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUFxH77js6Iy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjBSDCUTs6L9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM\n",
        "\n",
        "# 1. Load dataset\n",
        "df = pd.read_csv('/content/Walmart_Sales.csv', parse_dates=['Date'])\n",
        "df = df[['Weekly_Sales']]\n",
        "\n",
        "# 2. Scale data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_sales = scaler.fit_transform(df)\n",
        "\n",
        "# 3. Create sequences\n",
        "def create_sequences(dataset, window_size):\n",
        "    X, y = [], []\n",
        "    for i in range(window_size, len(dataset)):\n",
        "        X.append(dataset[i-window_size:i, 0])\n",
        "        y.append(dataset[i, 0])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "window_size = 30\n",
        "X, y = create_sequences(scaled_sales, window_size)\n",
        "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# 4. Split into training and testing\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_test, y_test = X[train_size:], y[train_size:]\n",
        "\n",
        "# 5. Build Simple RNN model\n",
        "model = Sequential([\n",
        "    LSTM(50, activation='tanh', input_shape=(window_size, 1)),\n",
        "    Dense(1)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.summary()\n",
        "\n",
        "# 6. Train\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# 7. Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = scaler.inverse_transform(y_pred)\n",
        "y_actual = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# 8. Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_actual, label='Actual Walmart Sales')\n",
        "plt.plot(y_pred, label='Predicted Walmart Sales')\n",
        "plt.title('Walmart Sales Prediction using Simple RNN')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTN-_Mp0s6PY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q-mAXLdDpWV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUK2NwpL4BYB"
      },
      "outputs": [],
      "source": [
        "# Sample text data\n",
        "text = \"hello world. This is an RNN based text generation example\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807OBtiR3g5B"
      },
      "outputs": [],
      "source": [
        "# create  character to index mappings\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
        "idx_to_char= {i:c for i, c in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dz5wnvWC3g7Y"
      },
      "outputs": [],
      "source": [
        "# convert text to sequence of integers\n",
        "seq = [char_to_idx[c] for c in text]\n",
        "vocab_size=len(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGIFkC7d3g-F"
      },
      "outputs": [],
      "source": [
        "# set sequence length\n",
        "seq_length = 10\n",
        "X = []\n",
        "y = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rwk3vdGU3hB4"
      },
      "outputs": [],
      "source": [
        "# Create input - output pairs\n",
        "for i in range(len(seq) - seq_length):\n",
        "  X.append(seq[i:i+seq_length])\n",
        "  y.append(seq[i+seq_length])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jpfb27P3hE1"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=16, input_length=seq_length))\n",
        "model.add(SimpleRNN(128, return_sequences=False))\n",
        "model.add(Dense(vocab_size, activation = 'softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcHiz40A7r3e"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZZeZHCz7sGt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "model.fit(X, y, epochs=30, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BpabdW008sn7"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_text, gen_length=100):\n",
        "  input_seq = [char_to_idx[c] for c in start_text[-seq_length:]]\n",
        "  generate = start_text\n",
        "\n",
        "  for _ in range(gen_length):\n",
        "    input_pad = np.array(input_seq[-seq_length:]).reshape(1, -1)\n",
        "    pred = model.predict(input_pad, verbose=0)\n",
        "    next_idx = np.argmax(pred)\n",
        "    next_char = idx_to_char[next_idx]\n",
        "    generated += next_char\n",
        "    input_seq.append(next_idx)\n",
        "\n",
        "  return generated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gtjwDf-8sq2"
      },
      "outputs": [],
      "source": [
        "seed_text = \"hello world\"\n",
        "print(generate_text(model, seed_text, gen_length=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75bz2c5j8svE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxJML7KZ8syI"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnUDcKH68s0v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a constant tensor of shape (2, 3)\n",
        "const_tensor = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"Constant Tensor:\")\n",
        "print(const_tensor)\n",
        "print(\"Data type of constant tensor:\", const_tensor.dtype)\n",
        "\n",
        "# Create a variable tensor of shape (2, 3)\n",
        "var_tensor = tf.Variable([[1, 2, 3], [4, 5, 6]])\n",
        "print(\"\\nVariable Tensor:\")\n",
        "print(var_tensor)\n",
        "print(\"Data type of variable tensor:\", var_tensor.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcQmWSdW8s3n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "a = tf.constant([[1,2,3], [4,5,6]])\n",
        "print(type(a))\n",
        "print(a.dtype)\n",
        "print(a.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4TjQoF-8s7A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize input data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "id": "9v-48lWc1Np5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize input data\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Define the model\n",
        "# Define the model\n",
        "model = Sequential([\n",
        "    LSTM(32, activation='tanh', input_shape=(28, 28)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "print(\"Test Loss:\", test_loss)\n"
      ],
      "metadata": {
        "id": "UT9HwTI01NtW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}